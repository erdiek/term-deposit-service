# Term Deposit Prediction Service

A production-ready Machine Learning service that predicts whether a banking customer will subscribe to a term deposit. This project implements a reproducible training pipeline, a FAST API, and containerization for deployment.

## Project Structure

```text
term-deposit-service/
├── data/               # Place dataset.csv here (ignored by git)
├── src/
│   ├── train.py        # Training pipeline (Preprocessing + Tuning + Modeling)
│   ├── app.py          # FastAPI inference service
│   ├── model.joblib    # Saved model artifact (generated by training)
│   └── metrics.json    # Local experiment tracking logs
├── Dockerfile          # Container definition
├── requirements.txt    # Python dependencies
└── README.md           # Documentation

## Prerequisites
Download the repository from GitHub
Important: The dataset is excluded from this repository for privacy. Before running the code, please ensure dataset.csv is placed inside the data/ directory.

OPTION 1: Local Environment
    Quick Start (Local Python Environment)
    Use this method if you do not have Docker installed.

    1. Install Dependencies Navigate to the root directory and install the required libraries. A virtual environment (Python 3.9+) is recommended.
    Run below bash code
        pip install -r requirements.txt
    2. Train the Model Run the training pipeline. This handles data loading, preprocessing, hyperparameter tuning, and artifact generation.
    Run below bash code
        cd src
        python train.py
        python app.py
    Output: Two files, model.joblib and metrics.json, will be created in the src folder.

    3. Run the Service Start the API server locally.
    Ensure you are inside the 'src' folder. Run below bash code
        uvicorn app:app --reload
    Note: If the command uvicorn is not found, try: python -m uvicorn app:app --reload

    4. Test the API Open your browser to the interactive documentation:
        URL: http://127.0.0.1:8000/docs
    Use the "Try it out" button on the /predict endpoint to send test data.

OPTION 2: Docker
    1. Build the Image
    Run from the root directory
        docker build -t term-deposit-service .

    2. Run the Container
    Run below bash code
        docker run -p 8000:8000 term-deposit-service
    3. Verify Access the API at http://localhost:8000/docs

# Rationale & Methodology

    Model Selection: Logistic Regression
        The main reasons I selected Logistic Regression over ensembe methods like Random Forest are explainability and efficiency.
        In Logistic Regression models we can see the coefficients of each feature or interaction features so that we can explain if we have a compliance audit.
        Additionally, having coefficents visible i will be easier for business intelligence and marketing team to analyse who to target.
        In terms of efficiency, the pipeline is lightweight, ensure fast training and inference.
    Experiment Tracking: I implemented a local JSON logger to demonstrate metadata tracking without requiring external cloud API keys for this assessment.


# Part A:
    ## Additional Questions:
    1. What sort of testing would you have on the training code?
        a. Unit Tests (Logic Verification)
        What: Test individual functions and custom transformers to ensure they behave deterministically.
        Example: Verify that the imputer correctly fills NaN values with the mean, or that the OneHotEncoder handles unseen categories without crashing (checking the handle_unknown='ignore' logic).
        b.Data Validation Tests (Pre-Training)
        What: Ensure the input data matches the expected schema before wasting compute resources on training.
        Example: Check that essential columns (like age, job, y) exist, that data types are correct (e.g., age is numeric), and that the dataset isn't empty.
    2. As you’re conducting experimentation to improve the model, how do you make sure the necessary configs, data, hyperparameters and evaluation outcome are stored?
        Ideal Production Workflow: In a live environment, I would replace the JSON logger with a dedicated platform like Weights & Biases (WandB) or the built-in experiment tracking of the cloud provider (e.g., Vertex AI Experiments or Azure ML). This would allow me to visualize the impact of every single run—comparing l1 vs l2 performance over time—and decide whether to promote a new model based on historical trends rather than a single snapshot.
        For this assessment, I used a hybrid experimentation approach tailored to the time constraints, while ensuring reproducibility via local logging.
        Hybrid Testing Strategy:
        Isolation Checks: I tested critical architectural decisions independently first. Specifically, I validated class_weight='balanced' vs None (finding that 'balanced' was essential for Recall) and interaction_only=False (confirming squared terms improved performance).
        Bulk Tuning: Once the architecture was set, I ran a bulk RandomizedSearchCV to simultaneously optimize the regularization strength (C) and penalty types (l1/l2).
        Rationale: In a full production cycle, I would run distinct experiments for each parameter sweep. However, given the timing requirements of this task, I condensed the fine-tuning into a single search execution to quickly converge on a viable candidate.
        Artifact Management (Local Store): To ensure the outcome wasn't lost in the terminal, I implemented a script to automatically capture the winning configuration. The training pipeline saves two artifacts for every successful run:
        Config & Metrics: stored in metrics.json (containing the best hyperparameters and the classification report).
        The Model: stored as model.joblib.
    3. You notice that your training results are different each time and fluctuates without you changing anything. What’s wrong? How do you fix it?
        What's wrong: The training pipeline involves non-deterministic operations. Even without code changes, fluctuations occur because algorithms rely on pseudo-random number generators for:
        Data Splitting: How rows are assigned to Train vs. Test sets.
        Initialization: How model weights or solver states are initialized.
        Optimization: Solvers (like SGD or coordinate descent) often shuffle data during training to escape local minima.
        How to fix it: We must enforce reproducibility by setting a fixed Random State (Seed) across the entire pipeline.
        Implementation: In my code, I explicitly passed random_state=42 to every stochastic component:
    4. How do you approach setting up a retraining strategy?
    Retraining shouldn't be an ad-hoc manual task. I would implement an automated Continuous Training (CT) pipeline triggered by two specific signals:
        a. The Triggers (When to retrain?)
        Schedule-Based: Banking data often has seasonal patterns (e.g., end-of-financial-year behavior). I would set up a scheduled job (e.g., monthly) to retrain the model on the most recent rolling window of data to capture these trends.
        Performance-Based: I would set up monitoring for Concept Drift. If the live model's performance (e.g., Recall) drops below a baseline threshold in production monitoring, it automatically triggers an out-of-cycle retraining job.

        b. The Champion/Challenger Evaluation Crucially, a newly trained model does not automatically go to production. It enters a "Challenger" state:
        The Champion: The current live model.
        The Challenger: The newly trained model.
        The Test: Both models are evaluated on a "Golden Dataset" (a holdout set of recent, verified data).
        The Decision: The Challenger only replaces the Champion if it shows a statistically significant improvement in key metrics (Recall/F1) without degrading latency.

        c. Implementation In a modern stack, I would orchestrate this using tools like Dagster. The pipeline would handle: Data Extraction -> Training -> Evaluation -> Model Registry Update (tagging the new model as 'Production' if it wins).

# Part B:
    ## Additional Questions:
    1. If there is a model already in production, how do you promote the model safely?
        My goal is to never break the user experience. I would avoid "Big Bang" releases (swapping 100% at once) and use a gradual rollout strategy:

        a. The "Secret" Test (Shadow Mode) First, I deploy the new model alongside the old one. It receives real data and makes predictions, but we don't show them to the customer.
        Reason: We can compare the new model against the old one safely. If the new model crashes or is too slow, no customer is affected. We just turn it off and fix it.

        b. The "Toe in the Water" (Canary Deployment) Once the model passes the secret test, I send a tiny amount of traffic (e.g., 5%) to the new model.
        Reason: If there is a hidden bug that only happens with real users, it only affects 5% of people, not everyone.

        c. The Safety Switch (Automated Rollback) I set up automatic alarms. If the new model starts throwing errors or running slowly, the system automatically switches everyone back to the old model instantly.
        Reason: It removes the need for a human to wake up at 3 AM to fix a broken update.
    2. You’re ready to deploy a new model, but the schema and preprocessing code is different to the current model in production - how would you handle this change? What would you do about the existing consumers of the API?
        If the new model requires different inputs (e.g., a new field), I treat this as a Breaking Change. I wouldn't simply overwrite the existing model because it can crash the current mobile app or website.

        The Strategy: API Versioning
            Open a "New Door" (Versioning): I would deploy the new model behind a new URL, like POST /v2/predict. The old model stays exactly where it is at POST /v1/predict.
            Reason: This guarantees Backward Compatibility. The existing app continues to work perfectly using the old "door" (v1), while we test the new one.
        Run Parallel Systems: Both v1 (Old Model) and v2 (New Model) run at the same time. They are completely separate code paths.
        The Migration (The Switch): I inform the app developers to update their code to send the new data to the v2 endpoint.
        Monitoring: I watch the traffic graphs. I should see v2 traffic rising and v1 traffic falling.
        Deprecation (Cleanup): Once the traffic to v1 drops to zero (meaning all users have updated their apps), I can safely delete the old code and turn off the old model.
    3. In an ideal world, what sort of observability metrics would you have on the API? When would you trigger an alert?
        I don't just check if the server is "on." I monitor other layers to ensure the system is actually working. Such as;
        a. System Health (Is it fast?)
        What I watch: Speed and Errors.
        The Metric: "Latency." How long does it take to give an answer? I look at the slowest 1% of requests (p99), because that's where customers get angry.
        Alert Trigger: IMMEDIATE. If the API starts crashing (Error Rate > 1%) or gets very slow (>500ms), page the on-call engineer immediately.

        b. Model Health (Is it smart?)
        What I watch: "Drift." Is the model acting differently than it did in training?
        The Metric: "Prediction Rate." If the model historically predicts "Yes" 10% of the time, but suddenly predicts "Yes" 80% of the time, it's broken.
        Alert Trigger: WARNING. Send a Slack message to the Data Science team. This usually isn't an emergency at 3 AM, but it needs investigation the next morning.

        c. Business Health (Is it useful?)
        What I watch: Reality.
        The Metric: "Conversion Rate." We predicted "Yes"—but did the customer actually open the term deposit?
        Why? If the model says "Yes" but customers say "No," the model is useless, even if the code works perfectly. 